"""
ä¸»æ§åˆ¶å™¨ - åè°ƒæ‰€æœ‰æ¨¡å—ï¼Œå®ç°å®Œæ•´å·¥ä½œæµ
"""

import asyncio
import logging
from pathlib import Path
from typing import List, Optional, Set
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TimeRemainingColumn

from .log_parser import LogParser
from .api_client import HyperliquidAPIClient
from .metrics_engine import MetricsEngine, AddressMetrics
from .data_store import DataStore, get_store
from .output_renderer import OutputRenderer

logger = logging.getLogger(__name__)


class Orchestrator:
    """ä¸»æ§åˆ¶å™¨ - åè°ƒæ•´ä¸ªåˆ†æå·¥ä½œæµ"""

    def __init__(
        self,
        log_path: str = "trades.log",
        force_refresh: bool = False,
        max_concurrent: int = 10,
        rate_limit: int = 50
    ):
        """
        åˆå§‹åŒ–ä¸»æ§åˆ¶å™¨

        Args:
            log_path: trades.log è·¯å¾„
            force_refresh: å¼ºåˆ¶åˆ·æ–°ï¼ˆå¿½ç•¥ç¼“å­˜ï¼‰
            max_concurrent: æœ€å¤§å¹¶å‘æ•°
            rate_limit: APIé€Ÿç‡é™åˆ¶
        """
        self.log_path = log_path
        self.force_refresh = force_refresh

        # åˆå§‹åŒ–ç»„ä»¶
        self.log_parser = LogParser(log_path)
        self.store: Optional[DataStore] = None
        self.api_client: Optional[HyperliquidAPIClient] = None
        self.metrics_engine = MetricsEngine()
        self.renderer = OutputRenderer()

        self.max_concurrent = max_concurrent
        self.rate_limit = rate_limit

        # é»‘åå•
        self.blacklist: Set[str] = set()
        self._load_blacklist()

    def _load_blacklist(self):
        """ä» blacklist.txt åŠ è½½é»‘åå•åœ°å€"""
        blacklist_path = Path(__file__).parent / "blacklist.txt"
        if not blacklist_path.exists():
            logger.info("æœªæ‰¾åˆ°é»‘åå•æ–‡ä»¶ï¼Œè·³è¿‡é»‘åå•è¿‡æ»¤")
            return
        with open(blacklist_path, "r") as f:
            for line in f:
                addr = line.strip().lower()
                if addr:
                    self.blacklist.add(addr)
        if self.blacklist:
            logger.info(f"å·²åŠ è½½é»‘åå•: {len(self.blacklist)} ä¸ªåœ°å€")

    async def initialize(self):
        """åˆå§‹åŒ–æ•°æ®åº“å’ŒAPIå®¢æˆ·ç«¯"""
        logger.info("========== å¼€å§‹åˆå§‹åŒ– ==========")
        logger.info("åˆå§‹åŒ–æ•°æ®å­˜å‚¨...")
        self.store = get_store()
        await self.store.connect(max_connections=self.max_concurrent * 2)
        logger.info(f"æ•°æ®åº“è¿æ¥æ± å·²å»ºç«‹: æœ€å¤§è¿æ¥æ•° {self.max_concurrent * 2}")

        logger.info("åˆå§‹åŒ–APIå®¢æˆ·ç«¯...")
        self.api_client = HyperliquidAPIClient(
            store=self.store,
            max_concurrent=self.max_concurrent,
            rate_limit=self.rate_limit
        )
        logger.info(f"APIå®¢æˆ·ç«¯å·²åˆå§‹åŒ–: æœ€å¤§å¹¶å‘ {self.max_concurrent}, é€Ÿç‡é™åˆ¶ {self.rate_limit} req/s")
        logger.info("========== åˆå§‹åŒ–å®Œæˆ ==========\n")

    async def cleanup(self):
        """æ¸…ç†èµ„æº"""
        logger.info("å¼€å§‹æ¸…ç†èµ„æº...")
        if self.store:
            await self.store.close()
            logger.info("æ•°æ®åº“è¿æ¥å·²å…³é—­")

    async def run(
        self,
        output_terminal: bool = True,
        output_html: bool = True,
        html_path: str = "output/analysis_report.html",
        terminal_path: Optional[str] = None,
        top_n: int = 50
    ) -> List[AddressMetrics]:
        """
        è¿è¡Œå®Œæ•´åˆ†ææµç¨‹

        Args:
            output_terminal: è¾“å‡ºç»ˆç«¯è¡¨æ ¼
            output_html: è¾“å‡ºHTMLæŠ¥å‘Š
            html_path: HTMLæŠ¥å‘Šè·¯å¾„
            terminal_path: ç»ˆç«¯è¾“å‡ºä¿å­˜è·¯å¾„ï¼ˆå¯é€‰ï¼‰
            top_n: ç»ˆç«¯æ˜¾ç¤ºå‰Nä¸ªåœ°å€

        Returns:
            æŒ‡æ ‡åˆ—è¡¨
        """
        try:
            # 1. è§£ææ—¥å¿—
            self.renderer.console.print("\n[bold cyan]æ­¥éª¤ 1/5:[/bold cyan] è§£æäº¤æ˜“æ—¥å¿—...")
            logger.info(f"æ­¥éª¤ 1/5: å¼€å§‹è§£æäº¤æ˜“æ—¥å¿—æ–‡ä»¶: {self.log_path}")
            address_stats = self.log_parser.parse()
            total_parsed = len(address_stats)
            logger.info(f"æ­¥éª¤ 1/5: è§£æåˆ° {total_parsed} ä¸ªå”¯ä¸€åœ°å€")

            # è¿‡æ»¤é»‘åå•åœ°å€
            if self.blacklist:
                before_count = len(address_stats)
                address_stats = {
                    addr: stats for addr, stats in address_stats.items()
                    if addr.lower() not in self.blacklist
                }
                blacklisted_count = before_count - len(address_stats)
                if blacklisted_count > 0:
                    logger.info(f"å·²è¿‡æ»¤é»‘åå•åœ°å€: {blacklisted_count} ä¸ª")
                    self.renderer.console.print(f"ğŸš« å·²è¿‡æ»¤é»‘åå•åœ°å€ [bold]{blacklisted_count}[/bold] ä¸ª\n")

            addresses = list(address_stats.keys())
            logger.info(f"æ­¥éª¤ 1/5 å®Œæˆ: æœ€ç»ˆ {len(addresses)} ä¸ªæœ‰æ•ˆåœ°å€")
            self.renderer.console.print(f"âœ… è§£æåˆ° [bold]{total_parsed}[/bold] ä¸ªå”¯ä¸€åœ°å€ï¼Œæœ‰æ•ˆ [bold]{len(addresses)}[/bold] ä¸ª\n")

            # 2. æ›´æ–°åœ°å€è¡¨
            self.renderer.console.print("[bold cyan]æ­¥éª¤ 2/5:[/bold cyan] æ›´æ–°åœ°å€æ•°æ®åº“...")
            logger.info(f"æ­¥éª¤ 2/5: å¼€å§‹æ›´æ–°åœ°å€æ•°æ®åº“ï¼Œå…± {len(addresses)} ä¸ªåœ°å€")
            await self.store.upsert_addresses([
                {
                    'address': addr,
                    'taker_count': stats['taker_count'],
                    'maker_count': stats['maker_count']
                }
                for addr, stats in address_stats.items()
            ])
            logger.info("æ­¥éª¤ 2/5 å®Œæˆ: åœ°å€æ•°æ®åº“å·²æ›´æ–°")
            self.renderer.console.print("âœ… åœ°å€æ•°æ®åº“å·²æ›´æ–°\n")

            # 3. è·å–å¾…å¤„ç†åœ°å€
            if self.force_refresh:
                pending_addresses = addresses
                logger.info(f"å¼ºåˆ¶åˆ·æ–°æ¨¡å¼: å°†é‡æ–°è·å–æ‰€æœ‰ {len(addresses)} ä¸ªåœ°å€çš„æ•°æ®")
                self.renderer.console.print("[yellow]âš ï¸  å¼ºåˆ¶åˆ·æ–°æ¨¡å¼ï¼šå°†é‡æ–°è·å–æ‰€æœ‰åœ°å€æ•°æ®[/yellow]\n")
            else:
                pending_addresses = await self.store.get_pending_addresses()
                if not pending_addresses:
                    pending_addresses = addresses
                    logger.info(f"æœªæ‰¾åˆ°å¾…å¤„ç†åœ°å€ï¼Œå°†è·å–æ‰€æœ‰ {len(addresses)} ä¸ªåœ°å€çš„æ•°æ®")
                    self.renderer.console.print("[yellow]âš ï¸  æœªæ‰¾åˆ°å¾…å¤„ç†åœ°å€ï¼Œå°†è·å–æ‰€æœ‰åœ°å€æ•°æ®[/yellow]\n")
                else:
                    logger.info(f"å‘ç° {len(pending_addresses)} ä¸ªå¾…å¤„ç†åœ°å€ï¼ˆå…± {len(addresses)} ä¸ªåœ°å€ï¼‰")
                    self.renderer.console.print(f"ğŸ“‹ å‘ç° [bold]{len(pending_addresses)}[/bold] ä¸ªå¾…å¤„ç†åœ°å€\n")

            # 4. æ‰¹é‡è·å–APIæ•°æ®
            self.renderer.console.print(f"[bold cyan]æ­¥éª¤ 3/5:[/bold cyan] è·å–APIæ•°æ®ï¼ˆ{len(pending_addresses)} ä¸ªåœ°å€ï¼‰...")
            logger.info(f"æ­¥éª¤ 3/5: å¼€å§‹è·å–APIæ•°æ®ï¼Œå…± {len(pending_addresses)} ä¸ªå¾…å¤„ç†åœ°å€")

            # è¿›åº¦è®¡æ•°å™¨
            processed_count = 0
            success_count = 0
            failed_count = 0

            with Progress(
                SpinnerColumn(),
                TextColumn("[progress.description]{task.description}"),
                BarColumn(),
                TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
                TimeRemainingColumn(),
                console=self.renderer.console
            ) as progress:
                task = progress.add_task("æ­£åœ¨è·å–æ•°æ®...", total=len(pending_addresses))

                async def process_address(addr: str, index: int) -> Optional[dict]:
                    """å¤„ç†å•ä¸ªåœ°å€"""
                    nonlocal processed_count, success_count, failed_count

                    try:
                        logger.info(f"[{index}/{len(pending_addresses)}] å¼€å§‹å¤„ç†åœ°å€: {addr}")

                        # æ›´æ–°çŠ¶æ€ä¸ºå¤„ç†ä¸­
                        await self.store.update_processing_status(addr, 'processing')

                        # è·å–æ•°æ®
                        data = await self.api_client.fetch_address_data(addr, save_to_db=True)

                        # æ›´æ–°çŠ¶æ€ä¸ºå®Œæˆ
                        await self.store.update_processing_status(addr, 'completed')

                        # æ ‡è®°åœ°å€æ•°æ®å·²å®Œæ•´è·å–
                        await self.store.mark_address_complete(addr)

                        processed_count += 1
                        success_count += 1
                        progress.advance(task)

                        logger.info(
                            f"[{index}/{len(pending_addresses)}] âœ… æˆåŠŸå¤„ç†: {addr} "
                            f"(å·²å¤„ç†: {processed_count}, æˆåŠŸ: {success_count}, å¤±è´¥: {failed_count})"
                        )
                        return data

                    except Exception as e:
                        processed_count += 1
                        failed_count += 1
                        logger.error(
                            f"[{index}/{len(pending_addresses)}] âŒ å¤„ç†å¤±è´¥: {addr[:10]}... - {e} "
                            f"(å·²å¤„ç†: {processed_count}, æˆåŠŸ: {success_count}, å¤±è´¥: {failed_count})"
                        )
                        await self.store.update_processing_status(addr, 'failed', str(e))
                        progress.advance(task)
                        return None

                # å¹¶å‘å¤„ç†ï¼ˆä¸ºæ¯ä¸ªåœ°å€åˆ†é…ç´¢å¼•ï¼‰
                tasks = [process_address(addr, idx + 1) for idx, addr in enumerate(pending_addresses)]
                results = await asyncio.gather(*tasks, return_exceptions=True)

                # è¿‡æ»¤å¤±è´¥ç»“æœ
                successful_results = [r for r in results if r and not isinstance(r, Exception)]

            logger.info(f"æ­¥éª¤ 3/5 å®Œæˆ: å…±å¤„ç† {processed_count} ä¸ªåœ°å€ï¼ŒæˆåŠŸ {success_count} ä¸ªï¼Œå¤±è´¥ {failed_count} ä¸ª")

            self.renderer.console.print(f"âœ… æˆåŠŸè·å– [bold]{len(successful_results)}[/bold] ä¸ªåœ°å€çš„æ•°æ®\n")

            # API ç»Ÿè®¡
            stats = self.api_client.get_stats()
            logger.info(
                f"API ç»Ÿè®¡: æ€»è¯·æ±‚ {stats['total_requests']} æ¬¡, "
                f"ç¼“å­˜å‘½ä¸­ {stats['cache_hits']} æ¬¡ (å‘½ä¸­ç‡: {stats['cache_hit_rate']:.1%}), "
                f"APIé”™è¯¯ {stats['api_errors']} æ¬¡"
            )
            self.renderer.console.print(
                f"[dim]API ç»Ÿè®¡: è¯·æ±‚ {stats['total_requests']} æ¬¡, "
                f"ç¼“å­˜å‘½ä¸­ {stats['cache_hits']} æ¬¡ ({stats['cache_hit_rate']:.1%}), "
                f"é”™è¯¯ {stats['api_errors']} æ¬¡[/dim]\n"
            )

            # 5. è®¡ç®—æŒ‡æ ‡
            self.renderer.console.print(f"[bold cyan]æ­¥éª¤ 4/5:[/bold cyan] è®¡ç®—äº¤æ˜“æŒ‡æ ‡...")
            logger.info(f"æ­¥éª¤ 4/5: å¼€å§‹è®¡ç®—äº¤æ˜“æŒ‡æ ‡ï¼Œå…± {len(addresses)} ä¸ªåœ°å€")

            all_metrics = []
            calculated_count = 0
            qualified_count = 0
            skipped_no_fills = 0
            skipped_few_fills = 0
            skipped_filters = 0
            skipped_liquidation = 0

            for idx, addr in enumerate(addresses, 1):
                logger.info(f"[{idx}/{len(addresses)}] è®¡ç®—æŒ‡æ ‡: {addr}")

                # æ£€æŸ¥æœ€è¿‘1å‘¨æ˜¯å¦æœ‰çˆ†ä»“è®°å½•ï¼ˆä¼˜å…ˆæ£€æŸ¥ï¼Œé¿å…æ— æ•ˆè®¡ç®—ï¼‰
                has_recent_liq = await self.store.has_recent_liquidation(addr, days=7)
                if has_recent_liq:
                    skipped_liquidation += 1
                    logger.warning(f"[{idx}/{len(addresses)}] åœ°å€ {addr[:10]}... æœ€è¿‘1å‘¨æœ‰çˆ†ä»“è®°å½•ï¼Œè·³è¿‡åˆ†æ")
                    continue

                # ä»æ•°æ®åº“è¯»å–äº¤æ˜“è®°å½•
                fills = await self.store.get_fills(addr)
                if not fills:
                    skipped_no_fills += 1
                    logger.warning(f"[{idx}/{len(addresses)}] åœ°å€æ— äº¤æ˜“è®°å½•: {addr[:10]}... (è·³è¿‡)")
                    continue

                if len(fills) < 10:
                    skipped_few_fills += 1
                    logger.warning(f"[{idx}/{len(addresses)}] åœ°å€ {addr[:10]}... å†å²è®¢å•ä»… {len(fills)} ç¬”ï¼ˆ<10ï¼‰ï¼Œè·³è¿‡åˆ†æ")
                    continue

                # è·å–è´¦æˆ·çŠ¶æ€ï¼ˆä»æ•°æ®åº“ï¼‰
                state = await self.store.get_latest_user_state(addr)

                # è·å– Spot è´¦æˆ·çŠ¶æ€ï¼ˆä»æ•°æ®åº“ï¼‰
                spot_state = await self.store.get_latest_spot_state(addr)

                # è·å–å‡ºå…¥é‡‘ç»Ÿè®¡
                transfer_stats = await self.store.get_net_deposits(addr)

                # è®¡ç®—æŒ‡æ ‡ï¼ˆä¼ å…¥æ–°å‚æ•°ï¼ŒåŒ…æ‹¬ spot_stateï¼‰
                metrics = self.metrics_engine.calculate_metrics(
                    address=addr,
                    fills=fills,
                    state=state,
                    transfer_data=transfer_stats,
                    spot_state=spot_state
                )

                # ä¿å­˜åˆ°ç¼“å­˜
                await self.store.save_metrics(addr, {
                    'total_trades': metrics.total_trades,
                    'win_rate': metrics.win_rate,
                    'total_pnl': metrics.total_pnl,
                })

                calculated_count += 1

                # ç­›é€‰æ¡ä»¶
                if metrics.total_pnl < 0:
                    skipped_filters += 1
                    logger.warning(f"[{idx}/{len(addresses)}] åœ°å€ {addr[:10]}... æ€»PNL<0ï¼Œè·³è¿‡æŠ¥å‘Šè¾“å‡º")
                    continue
                if metrics.win_rate < 60:
                    skipped_filters += 1
                    logger.warning(f"[{idx}/{len(addresses)}] åœ°å€ {addr[:10]}... èƒœç‡<60%ï¼Œè·³è¿‡æŠ¥å‘Šè¾“å‡º")
                    continue

                qualified_count += 1
                all_metrics.append(metrics)
                logger.info(
                    f"[{idx}/{len(addresses)}] âœ… åœ°å€ç¬¦åˆæ¡ä»¶: {addr[:10]}... "
                    f"(PNL: {metrics.total_pnl:.2f}, èƒœç‡: {metrics.win_rate:.1f}%)"
                )

            logger.info(
                f"æ­¥éª¤ 4/5 å®Œæˆ: å…±è®¡ç®— {calculated_count} ä¸ªåœ°å€ï¼Œ"
                f"ç¬¦åˆæ¡ä»¶ {qualified_count} ä¸ªï¼Œ"
                f"æœ€è¿‘çˆ†ä»“è·³è¿‡ {skipped_liquidation} ä¸ªï¼Œ"
                f"æ— äº¤æ˜“è®°å½• {skipped_no_fills} ä¸ªï¼Œ"
                f"è®¢å•ä¸è¶³10ç¬”è·³è¿‡ {skipped_few_fills} ä¸ªï¼Œ"
                f"ä¸ç¬¦åˆç­›é€‰æ¡ä»¶ {skipped_filters} ä¸ª"
            )
            self.renderer.console.print(f"âœ… è®¡ç®—å®Œæˆ [bold]{len(all_metrics)}[/bold] ä¸ªåœ°å€çš„æŒ‡æ ‡\n")

            # 6. è¾“å‡ºæŠ¥å‘Š
            self.renderer.console.print("[bold cyan]æ­¥éª¤ 5/5:[/bold cyan] ç”ŸæˆæŠ¥å‘Š...\n")
            logger.info(f"æ­¥éª¤ 5/5: å¼€å§‹ç”ŸæˆæŠ¥å‘Šï¼Œå…± {len(all_metrics)} ä¸ªç¬¦åˆæ¡ä»¶çš„åœ°å€")

            if output_terminal:
                logger.info(f"ç”Ÿæˆç»ˆç«¯æŠ¥å‘Š: æ˜¾ç¤ºå‰ {top_n} ä¸ªåœ°å€")
                if terminal_path:
                    logger.info(f"ç»ˆç«¯æŠ¥å‘Šå°†ä¿å­˜åˆ°: {terminal_path}")
                self.renderer.render_terminal(all_metrics, top_n=top_n, save_path=terminal_path)

            if output_html and all_metrics:
                logger.info(f"ç”ŸæˆHTMLæŠ¥å‘Š: {html_path}")
                self.renderer.render_html(all_metrics, output_path=html_path)
                logger.info(f"HTMLæŠ¥å‘Šå·²ä¿å­˜: {html_path}")

            logger.info(
                f"========== åˆ†æå®Œæˆ ==========\n"
                f"æ€»åœ°å€æ•°: {len(addresses)}\n"
                f"å¾…å¤„ç†åœ°å€: {len(pending_addresses)}\n"
                f"æˆåŠŸè·å–æ•°æ®: {success_count}\n"
                f"è·å–å¤±è´¥: {failed_count}\n"
                f"æœ€è¿‘çˆ†ä»“è·³è¿‡: {skipped_liquidation}\n"
                f"è®¡ç®—æŒ‡æ ‡: {calculated_count}\n"
                f"ç¬¦åˆç­›é€‰æ¡ä»¶: {qualified_count}\n"
                f"æœ€ç»ˆæŠ¥å‘Šåœ°å€æ•°: {len(all_metrics)}\n"
                f"=============================="
            )
            self.renderer.console.print("\n[bold green]âœ¨ åˆ†æå®Œæˆï¼[/bold green]\n")

            return all_metrics

        except Exception as e:
            logger.error(f"åˆ†ææµç¨‹å¤±è´¥: {e}", exc_info=True)
            self.renderer.console.print(f"\n[bold red]âŒ é”™è¯¯: {e}[/bold red]\n")
            raise
